# script parameters
model_id: "meta-llama/Llama-3.1-70B-Instruct" # Hugging Face model id
dataset_path: "./data/pileNER/5pos_5neg_perNE_top391NEs_TrueDef"
max_seq_length: 768  # 2048              # max sequence length for model and packing of the dataset
# training parameters
output_dir: "./SLIMER-llama-3-70b"     # Temporary output directory for model checkpoints
report_to: "tensorboard"               # report metrics to tensorboard
learning_rate: 0.0002                  # learning rate 2e-4
lr_scheduler_type: "constant"          # learning rate scheduler
num_train_epochs: 1                    # number of training epochs
per_device_train_batch_size: 1         # batch size per device during training
per_device_eval_batch_size: 1          # batch size for evaluation
gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass
optim: adamw_torch                     # use torch adamw optimizer
logging_steps: 10                      # log every 10 steps
save_strategy: epoch                   # save checkpoint every epoch
eval_strategy: steps                   # evaluate every epoch
eval_steps: 200
max_grad_norm: 0.3                     # max gradient norm
warmup_ratio: 0.03                     # warmup ratio
bf16: true                             # use bfloat16 precision
tf32: true                             # use tf32 precision
#gradient_checkpointing: true           # use gradient checkpointing to save memory
# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp

fsdp_config:
  fsdp_sharding_strategy: FULL_SHARD               # Full sharding of gradients, params, optimizer states
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP    # Automatically wrap transformer layers
  fsdp_backward_prefetch_policy: false     # Prefetch for backward pass (optimize memory usage)
  fsdp_forward_prefetch: false                     # Disable forward prefetching
  fsdp_cpu_ram_efficient_loading: true             # Load model efficiently to CPU RAM and shard across GPUs
  fsdp_offload_params: true                        # Offload params/gradients to CPU when not in use
  fsdp_state_dict_type: SHARDED_STATE_DICT         # Use sharded state dict for saving memory
  fsdp_sync_module_states: true                    # Ensure parameter synchronization across processes
  fsdp_transformer_layer_cls_to_wrap: BertLayer, LlamaDecoderLayer  # Specific layers for transformer-based models
  fsdp_use_orig_params: true                       # Necessary for QLoRA fine-tuning with frozen layers
